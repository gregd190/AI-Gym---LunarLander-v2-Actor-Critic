{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C Solution to OpenAI Gym LunarLander-v2 environment\n",
    "\n",
    "## Introduction:\n",
    "    \n",
    "### OpenAI Gym Environment Description:\n",
    "\"Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n",
    "\"\n",
    "\n",
    "### Discussion\n",
    "Due to the continuous and varied nature of the state space, discretising the state space at a sufficiently high resolution would result in an impractically large number of possible states. A conventional Q-table type solution is therefore impractical.\n",
    "An actor - Critic method is used, training 2 neural networks, an 'actor' network to determine the optimal action, and a 'critic' network to estimate the potential reward of the action. \n",
    "\n",
    "Keras, as a frontend for Tensorflow, is used to create and train the neural networks. \n",
    "\n",
    "As 'solved' is considered 200 points averaged over 100 episodes, the networks will be trained and optimized until the average over the last 100 iterations exceeds this value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import the various gym, keras, numpy and libraries we will require\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "import gym.wrappers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from collections import deque\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from multiprocessing import Pool, freeze_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the models\n",
    "\n",
    "Functions for model creation allow for flexibility in network size to allow for comparison of network sizes. \n",
    "\n",
    "Adam is used as the optimizer, as it has proven efficient on prior problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_critic(num_input_nodes, num_output_nodes, lr = 0.001, size = [256]):\n",
    "\t\n",
    "\tmodel = Sequential()\n",
    "\t\n",
    "\tmodel.add(Dense(size[0], input_shape = (8,), activation = 'relu'))\n",
    "\t\n",
    "\tfor i in range(1,len(size)):\n",
    "\t\tmodel.add(Dense(size[i], activation = 'relu'))\n",
    "\t\n",
    "    \n",
    "\tmodel.add(Dense(num_output_nodes, activation = 'linear')) \n",
    "\t\n",
    "\tadam = optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999)\n",
    "\t\n",
    "\tmodel.compile(loss = 'mse', optimizer = adam)\n",
    "\t\n",
    "\t#print('Critic Model Summary:')\n",
    "\t#model.summary()\n",
    "\t\n",
    "\treturn model\n",
    "\n",
    "def build_model_actor(num_input_nodes, num_output_nodes, lr = 0.001, size = [256]):\n",
    "\t\n",
    "\tmodel = Sequential()\n",
    "\t\n",
    "\tmodel.add(Dense(size[0], input_shape = (num_input_nodes,), activation = 'relu'))\n",
    "\t\n",
    "\tfor i in range(1, len(size)):\n",
    "\t\tmodel.add(Dense(size[i], activation = 'relu'))\n",
    "\t\n",
    "\tmodel.add(Dense(num_output_nodes, activation = 'softmax')) \n",
    "\t\n",
    "\tadam = optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999)\n",
    "\t\n",
    "\tmodel.compile(loss = 'categorical_crossentropy', optimizer = adam)\n",
    "\t\n",
    "\t#print('Actor Model Summary:')\n",
    "\t#model.summary()\n",
    "\t\n",
    "\treturn model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding on an Action\n",
    "\n",
    "Action state is very simple - one of 4 possible actions (do nothing, or fire left, right or main engine). Action is selected randomly from the 4 actions, with the probability of a given action being chosen being proportional to the probability the actor network give for that action being the optimal action. This inherently encourages exploration in the early stages of training, and moves to a exploitation strategy as the network becomes more sure of itself. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decide_action(actor, state):\n",
    "\n",
    "\tflat_state = np.reshape(state, [1,8])\n",
    "\taction = np.random.choice(4, 1, p = actor.predict(flat_state)[0])[0]\n",
    "\t\n",
    "\treturn(action)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running episodes\n",
    "\n",
    "The simulation is run for a predefined number of episodes.\n",
    "\n",
    "For each step, the state, action, resulting state, reward and whether or not the step completed the episode (the boolean 'done') were saved in a list 'memory'.\n",
    "\n",
    "For each episode the totalreward is saved in an array 'totrewardarray'.\n",
    "\n",
    "Each episode is limited to 1000 timesteps, to cut short scenarios where the lander (which contains infinite fuel) refusing to land in the early stages of training.\n",
    "\n",
    "The episodes run until either the predefined number of episodes are completed, or the problem is considered solved (average totalreward of last 100 episodes exceeds 200). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_episode(env, actor, r = False):\n",
    "    \n",
    "    memory = []\n",
    "    \n",
    "    bestyet = float('-inf')\n",
    "            \n",
    "    state = env.reset()\n",
    "\n",
    "    episode_reward = 0\n",
    "\n",
    "    cnt = 0 \n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done and cnt <1000:\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "        if r:\n",
    "            env.render()\n",
    "\n",
    "        action = decide_action(actor, state)\n",
    "        observation, reward, done, _ = env.step(action)  \n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        state_new = observation \n",
    "\n",
    "        memory.append((state, action, reward, state_new, done))\n",
    "\n",
    "        state = state_new \n",
    "\n",
    "    return(memory, episode_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Networks\n",
    "\n",
    "Now the memory list gathered from running the episodes to a training function which trains the networks. \n",
    "\n",
    "The training data is shuffled so it is not presented to the networks in order. \n",
    "\n",
    "The discount factor, 'gamma', is another hyperparameter that will need to be optimised. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_models(actor, critic, memory, gamma):\n",
    "\n",
    "\trandom.shuffle(memory)\n",
    "\t\n",
    "\tfor i in range(len(memory)):\n",
    "\n",
    "\t\tstate, action, reward, state_new, done = memory[i]\n",
    "\t\t\t\n",
    "\t\tflat_state_new = np.reshape(state_new, [1,8])\n",
    "\t\tflat_state = np.reshape(state, [1,8])\n",
    "\t\t\n",
    "\t\ttarget = np.zeros((1, 1))\n",
    "\t\tadvantages = np.zeros((1, 4))\n",
    "\n",
    "\t\tvalue = critic.predict(flat_state)\n",
    "\t\tnext_value = critic.predict(flat_state_new)\n",
    "\n",
    "\t\tif done:\n",
    "\t\t\tadvantages[0][action] = reward - value\n",
    "\t\t\ttarget[0][0] = reward\n",
    "\t\telse:\n",
    "\t\t\tadvantages[0][action] = reward + gamma * (next_value) - value\n",
    "\t\t\ttarget[0][0] = reward + gamma * next_value\n",
    "\t\t\n",
    "\t\tactor.fit(flat_state, advantages, epochs=1, verbose=0)\n",
    "\t\tcritic.fit(flat_state, target, epochs=1, verbose=0)\t\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running episodes without training\n",
    "\n",
    "Sometimes we might want to run episodes without saving data for training, for instance if we want to render a few episodes of the trained network, or if we want to assess the performance of a trained network. This is simply a modification of the 'run_episodes' function. \n",
    "\n",
    "It includes a render option (boolean 'r') which turns on or off rendering the episode. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_game(iters, r = True):\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    totalrewardarray = []\n",
    "    for i in range(iters):\n",
    "    \n",
    "        state = env.reset()\n",
    "        totalreward = 0\n",
    "        cnt = 0\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done and cnt <1000:\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "            if r:\n",
    "                import PIL\n",
    "                PIL.Image.fromarray(env.render(mode='rgb_array')).resize((320, 420))\n",
    "\n",
    "            action = decide_action(actor, state)\n",
    "\n",
    "            observation, reward, done, _ = env.step(action)  \n",
    "\n",
    "            totalreward += reward\n",
    "\n",
    "            state_new = observation \n",
    "\n",
    "            state = state_new\n",
    "            \n",
    "        totalrewardarray.append(totalreward)\n",
    "\n",
    "    return totalrewardarray\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "With the necessary building blocks in place, it is time to run some episodes and see how it performs. \n",
    "This function runs the episodes, trains the models on the episode data, and calculates the average performance over\n",
    "previous 100 episodes. If the average performance is the best yet, it saves the models. Finally, it plots the average reward \n",
    "vs number of episodes used for training. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_train_plot(alr, clr, gamma, numepisodes):\n",
    "    \n",
    "    env = gym.make('LunarLander-v2')\n",
    "  \n",
    "    i = 0\n",
    "\n",
    "    actor = build_model_actor(num_input_nodes = 8, num_output_nodes = 4, lr = alr, size = [64,64,64])\n",
    "    critic = build_model_critic(num_input_nodes = 8, num_output_nodes = 1, lr= clr, size = [64,64,64])\n",
    "\n",
    "    totrewardarray = [] #For storing the total reward from each episode\n",
    "\n",
    "    best = float('-inf') #For storing the best rolling average reward\n",
    "\n",
    "    episodes = len(totrewardarray) #Counting how many episodes have passed\n",
    "\n",
    "    while episodes < numepisodes:   \n",
    "\n",
    "        i+= 1\n",
    "\n",
    "        memory, episode_reward = run_episode(env, actor, r = False)\n",
    "\n",
    "        totrewardarray.append(episode_reward)\n",
    "\n",
    "        episodes = len(totrewardarray)\n",
    "\n",
    "        if episodes >= 100:\n",
    "            score = np.average(totrewardarray[-100:-1])\n",
    "            if score > best:\n",
    "                best = score\n",
    "                actor.save('actormodel.h5')\n",
    "                critic.save('criticmodel.h5')\n",
    "            if episodes%500==0:\n",
    "                print('ALR:', alr, ' CLR:', clr, 'episode ', episodes, 'of',numepisodes, 'Average Reward (last 100 eps)= ', score)\n",
    "\n",
    "        train_models(actor, critic, memory, gamma)\n",
    "\n",
    "        avgarray = []\n",
    "        cntarray = []\n",
    "\n",
    "    for i in range(100,len(totrewardarray),10):\n",
    "        avgarray.append(np.average(totrewardarray[i-100:i]))\n",
    "        cntarray.append(i)\n",
    "\n",
    "    plt.plot(cntarray, avgarray, label = 'Best 100 ep av. reward = '+str(best))\n",
    "        \n",
    "    plt.title('Rolling Average (previous 100) vs Iterations')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.show()\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grid search (not shown) found the following hyperparameters produced an average per-episode reward of more than 200 over 100 episodes after less than 5000 episodes of training:\n",
    "Actor Learning Rate = 5e-6\n",
    "Critic Learning Rate = 5e-4\n",
    "Gamma Value of 0.999\n",
    "Neural Network Size [64,64,64] (both networks)\n",
    "\n",
    "Note: If you do not wish to train the model from scratch, do no run the next section. The following section loads the best saved weights (included in the repository). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "ALR: 5e-06  CLR: 0.0005 episode  500 of 5000 Average Reward (last 100 eps)=  -186.472727999\n",
      "ALR: 5e-06  CLR: 0.0005 episode  1000 of 5000 Average Reward (last 100 eps)=  -191.85582657\n",
      "ALR: 5e-06  CLR: 0.0005 episode  1500 of 5000 Average Reward (last 100 eps)=  -139.905073712\n",
      "ALR: 5e-06  CLR: 0.0005 episode  2000 of 5000 Average Reward (last 100 eps)=  -116.953608546\n"
     ]
    }
   ],
   "source": [
    "run_train_plot(5e-6, 5e-4, 0.999, 5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the performance of the model reached a plateau after approximately 4000 episodes- It is unlikely that training over more episodes will provide greater performance. \n",
    "\n",
    "### Reviewing Performance\n",
    "\n",
    "Now to lets assess the performance of the trained model. Firstly, let's load the weights of the model with the best recorded performance during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the saved model at its best performance\n",
    "actor=load_model('actormodel.h5')\n",
    "critic=load_model('criticmodel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test the model over many episodes and view the distribution of rewards received per episode. \n",
    "Rendering has been disabled as it is very slow, however if you wish to watch, setting r=True will render the graphics (you may want to reduce the number of iterations). Note rendering may require additional/different libraries to be installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards = play_game(iters = 1000, r = False)\n",
    "plt.hist(rewards, 40, rwidth=0.8)\n",
    "plt.title('Performance of trained models')\n",
    "plt.xlabel('Episode reward')\n",
    "plt.ylabel('Number of Occurrences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noted that while the average performance is reasonably good, there are still a small fraction of episodes which result in unsatisactory landings. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
